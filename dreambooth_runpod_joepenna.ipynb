{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b495433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dreambooth\n",
    "### Notebook implementation by Joe Penna (@MysteryGuitarM on Twitter) - Improvements by David Bielejeski and Kane Wallmann\n",
    "\n",
    "### Instructions\n",
    "- Sign up for RunPod here: https://runpod.io/?ref=n8yfwyum\n",
    "    - Note: That's my personal referral link. Please don't use it if we are mortal enemies.\n",
    "\n",
    "- Click *Deploy* on either `SECURE CLOUD` or `COMMUNITY CLOUD`\n",
    "\n",
    "- Follow the rest of the instructions in this video: https://www.youtube.com/watch?v=7m__xadX0z0#t=5m33.1s\n",
    "\n",
    "Latest information on:\n",
    "https://github.com/JoePenna/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0427db8",
   "metadata": {},
   "source": [
    "## Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2AsGA1xpNQnb",
   "metadata": {
    "id": "2AsGA1xpNQnb"
   },
   "outputs": [],
   "source": [
    "# If running on Vast.AI, copy the code in this cell into a new notebook. Run it, then launch the `dreambooth_runpod_joepenna.ipynb` notebook from the jupyter interface.\n",
    "!git clone https://github.com/JoePenna/Dreambooth-Stable-Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1bc458-091b-42f4-a125-c3f0df20f29d",
   "metadata": {
    "id": "9e1bc458-091b-42f4-a125-c3f0df20f29d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BUILD ENV\n",
    "!pip install omegaconf\n",
    "!pip install einops\n",
    "!pip install pytorch-lightning==1.6.5\n",
    "!pip install test-tube\n",
    "!pip install transformers\n",
    "!pip install kornia\n",
    "!pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "!pip install setuptools==59.5.0\n",
    "!pip install pillow==9.0.1\n",
    "!pip install torchmetrics==0.6.0\n",
    "!pip install -e .\n",
    "!pip install protobuf==3.20.1\n",
    "!pip install gdown\n",
    "!pip install pydrive\n",
    "!pip install -qq diffusers[\"training\"]==0.3.0 transformers ftfy\n",
    "!pip install -qq \"ipywidgets>=7,<8\"\n",
    "!pip install huggingface_hub\n",
    "!pip install ipywidgets==7.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae11c10",
   "metadata": {
    "id": "dae11c10"
   },
   "outputs": [],
   "source": [
    "## Login to stable diffusion\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ae640",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the 1.4 sd model\n",
    "from huggingface_hub import hf_hub_download\n",
    "downloaded_model_path = hf_hub_download(\n",
    " repo_id=\"CompVis/stable-diffusion-v-1-4-original\",\n",
    " filename=\"sd-v1-4.ckpt\",\n",
    " use_auth_token=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Move the sd-v1-4.ckpt to the root of this directory as \"model.ckpt\"\n",
    "actual_locations_of_model_blob = !readlink -f {downloaded_model_path}\n",
    "!mv {actual_locations_of_model_blob[-1]} model.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1d11a",
   "metadata": {
    "id": "17d1d11a"
   },
   "source": [
    "# Regularization Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07a5df",
   "metadata": {
    "id": "ed07a5df"
   },
   "source": [
    "Training teaches your new model both your token **but** re-trains your class simultaneously.\n",
    "\n",
    "From cursory testing, it does not seem like reg images affect the model too much. However, they do affect your class greatly, which will in turn affect your generations.\n",
    "\n",
    "The regularization images should be of the format `class/class_xyz.jpg` (e.g. man/man_001.jpg, woman/woman_001.jpg, etc.). You can create your own or use the pre-generated ones below.\n",
    "\n",
    "The name of the sub-directory will be used to pair the training images against a related regularization image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mxPL2O0OLvBW",
   "metadata": {
    "id": "mxPL2O0OLvBW"
   },
   "source": [
    "## Download pre-generated regularization images\n",
    "\n",
    "I have prepared a repository which contains 1000 men, woman and person regularization images named correctly. This appears to work quite well at avoiding distortions if you are training photos of people. These were taken from djbielejeski/Stable-Diffusion-Regularization-Images-{dataset}.git repositories:\n",
    "\n",
    "* man_euler - provided by Niko Pueringer (Corridor Digital) - euler @ 40 steps, CFG 7.5\n",
    "* person_ddim\n",
    "* woman_ddim - provided by David Bielejeski - ddim @ 50 steps, CFG 10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7EydXCjOV1v",
   "metadata": {
    "id": "e7EydXCjOV1v"
   },
   "outputs": [],
   "source": [
    "# Grab the existing regularization images\n",
    "!git clone https://github.com/kanewallmann/Stable-Diffusion-Regularization-Images.git\n",
    "\n",
    "!mkdir -p outputs/txt2img-samples/regularization\n",
    "!mv -v Stable-Diffusion-Regularization-Images/* outputs/txt2img-samples/regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zshrC_JuMXmM",
   "metadata": {
    "id": "zshrC_JuMXmM"
   },
   "source": [
    "# Upload your training images\n",
    "Upload 10-20 images of someone to\n",
    "\n",
    "```\n",
    "/workspace/Dreambooth-Stable-Diffusion/training_samples\n",
    "```\n",
    "\n",
    "WARNING: Be sure to upload an *even* amount of images, otherwise the training inexplicably stops at 1500 steps.\n",
    "\n",
    "*   2-3 full body\n",
    "*   3-5 upper body\n",
    "*   5-12 close-up on face\n",
    "\n",
    "The images should be:\n",
    "\n",
    "- as close as possible to the kind of images you're trying to make (most of the time, that means no selfies).\n",
    "- named in the following format `class/identifier class_xyz.jpg` (example: \"man/kwallmann man_001.jpg\", \"dog/bholly dog_001.jpg\", etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bf4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Add here the URLs to the images of the subject you are adding\n",
    "urls = [\n",
    " \"https://i.imgur.com/test1.png\",\n",
    " \"https://i.imgur.com/test2.png\",\n",
    " \"https://i.imgur.com/test3.png\",\n",
    " \"https://i.imgur.com/test4.png\",\n",
    " \"https://i.imgur.com/test5.png\",\n",
    " # You can add additional images here -- about 20-30 images in different \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download and check the images you have just added\n",
    "import os\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    " assert len(imgs) == rows*cols\n",
    "\n",
    " w, h = imgs[0].size\n",
    " grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    " grid_w, grid_h = grid.size\n",
    "\n",
    " for i, img in enumerate(imgs):\n",
    "  grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    " return grid\n",
    "\n",
    "def download_image(url):\n",
    " try:\n",
    "  response = requests.get(url)\n",
    " except:\n",
    "  return None\n",
    " return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "images = list(filter(None,[download_image(url) for url in urls]))\n",
    "save_path = \"./training_samples\"\n",
    "if not os.path.exists(save_path):\n",
    " os.mkdir(save_path)\n",
    "[image.save(f\"{save_path}/{i}.png\", format=\"png\") for i, image in enumerate(images)]\n",
    "image_grid(images, 1, len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e50df",
   "metadata": {
    "id": "ad4e50df"
   },
   "source": [
    "## Training\n",
    "\n",
    "If training a person or subject, keep an eye on your project's `logs/{folder}/images/train/samples_scaled_gs-00xxxx` generations.\n",
    "\n",
    "If training a style, keep an eye on your project's `logs/{folder}/images/train/samples_gs-00xxxx` generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
   "metadata": {
    "id": "6fa5dd66-2ca0-4819-907e-802e25583ae6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 23\n",
      "Running on GPUs 0,\n",
      "Loading model from nai-animefull-final-pruned.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 64, 64) = 16384 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'logit_scale', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'text_projection.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Restored from nai-animefull-final-pruned.ckpt with 197 missing and 200 unexpected keys\n",
      "Missing Keys: ['cond_stage_model.transformer.text_model.embeddings.position_ids', 'cond_stage_model.transformer.text_model.embeddings.token_embedding.weight', 'cond_stage_model.transformer.text_model.embeddings.position_embedding.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.0.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.1.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.2.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.3.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.4.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.5.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.6.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.7.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.8.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.9.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.10.layer_norm2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.k_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.v_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.q_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.self_attn.out_proj.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc1.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.mlp.fc2.bias', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.weight', 'cond_stage_model.transformer.text_model.encoder.layers.11.layer_norm2.bias', 'cond_stage_model.transformer.text_model.final_layer_norm.weight', 'cond_stage_model.transformer.text_model.final_layer_norm.bias']\n",
      "Unexpected Keys: ['state_dict', 'model_ema.decay', 'model_ema.num_updates', 'cond_stage_model.transformer.embeddings.position_ids', 'cond_stage_model.transformer.embeddings.token_embedding.weight', 'cond_stage_model.transformer.embeddings.position_embedding.weight', 'cond_stage_model.transformer.encoder.layers.0.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.0.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.0.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.0.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.0.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.0.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.0.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.0.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.0.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.0.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.0.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.0.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.0.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.0.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.0.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.0.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.1.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.1.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.1.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.1.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.1.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.1.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.1.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.1.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.1.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.1.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.1.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.1.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.1.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.1.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.1.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.1.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.2.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.2.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.2.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.2.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.2.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.2.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.2.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.2.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.2.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.2.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.2.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.2.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.2.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.2.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.2.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.2.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.3.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.3.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.3.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.3.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.3.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.3.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.3.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.3.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.3.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.3.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.3.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.3.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.3.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.3.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.3.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.3.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.4.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.4.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.4.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.4.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.4.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.4.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.4.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.4.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.4.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.4.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.4.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.4.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.4.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.4.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.4.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.4.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.5.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.5.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.5.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.5.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.5.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.5.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.5.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.5.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.5.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.5.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.5.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.5.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.5.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.5.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.5.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.5.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.6.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.6.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.6.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.6.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.6.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.6.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.6.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.6.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.6.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.6.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.6.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.6.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.6.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.6.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.6.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.6.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.7.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.7.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.7.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.7.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.7.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.7.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.7.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.7.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.7.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.7.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.7.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.7.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.7.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.7.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.7.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.7.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.8.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.8.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.8.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.8.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.8.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.8.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.8.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.8.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.8.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.8.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.8.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.8.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.8.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.8.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.8.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.8.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.9.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.9.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.9.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.9.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.9.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.9.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.9.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.9.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.9.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.9.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.9.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.9.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.9.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.9.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.9.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.9.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.10.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.10.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.10.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.10.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.10.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.10.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.10.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.10.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.10.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.10.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.10.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.10.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.10.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.10.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.10.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.10.layer_norm2.bias', 'cond_stage_model.transformer.encoder.layers.11.self_attn.k_proj.weight', 'cond_stage_model.transformer.encoder.layers.11.self_attn.k_proj.bias', 'cond_stage_model.transformer.encoder.layers.11.self_attn.v_proj.weight', 'cond_stage_model.transformer.encoder.layers.11.self_attn.v_proj.bias', 'cond_stage_model.transformer.encoder.layers.11.self_attn.q_proj.weight', 'cond_stage_model.transformer.encoder.layers.11.self_attn.q_proj.bias', 'cond_stage_model.transformer.encoder.layers.11.self_attn.out_proj.weight', 'cond_stage_model.transformer.encoder.layers.11.self_attn.out_proj.bias', 'cond_stage_model.transformer.encoder.layers.11.layer_norm1.weight', 'cond_stage_model.transformer.encoder.layers.11.layer_norm1.bias', 'cond_stage_model.transformer.encoder.layers.11.mlp.fc1.weight', 'cond_stage_model.transformer.encoder.layers.11.mlp.fc1.bias', 'cond_stage_model.transformer.encoder.layers.11.mlp.fc2.weight', 'cond_stage_model.transformer.encoder.layers.11.mlp.fc2.bias', 'cond_stage_model.transformer.encoder.layers.11.layer_norm2.weight', 'cond_stage_model.transformer.encoder.layers.11.layer_norm2.bias', 'cond_stage_model.transformer.final_layer_norm.weight', 'cond_stage_model.transformer.final_layer_norm.bias']\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/loggers/test_tube.py:105: LightningDeprecationWarning: The TestTubeLogger is deprecated since v1.5 and will be removed in v1.7. We recommend switching to the `pytorch_lightning.loggers.TensorBoardLogger` as an alternative.\n",
      "  rank_zero_deprecation(\n",
      "Monitoring val/loss_simple_ema as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/ghislaine_smallset2022-11-08T21-59-52_ghislaine/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 10, 'every_n_train_steps': 300}}\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "#### Data #####\n",
      "train, PersonalizedBatchBase, 4200\n",
      "validation, PersonalizedBase, 42\n",
      "accumulate_grad_batches = 1\n",
      "++++ NOT USING LR SCALING ++++\n",
      "Setting learning rate to 1.00e-06\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_start` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LatentDiffusion: Also optimizing conditioner params!\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M \n",
      "1 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "---------------------------------------------------------\n",
      "982 M     Trainable params\n",
      "83.7 M    Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,264.941 Total estimated model params size (MB)\n",
      "Project config\n",
      "model:\n",
      "  base_learning_rate: 1.0e-06\n",
      "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
      "  params:\n",
      "    reg_weight: 1.0\n",
      "    linear_start: 0.00085\n",
      "    linear_end: 0.012\n",
      "    num_timesteps_cond: 1\n",
      "    log_every_t: 200\n",
      "    timesteps: 1000\n",
      "    first_stage_key: image\n",
      "    cond_stage_key: caption\n",
      "    image_size: 64\n",
      "    channels: 4\n",
      "    cond_stage_trainable: true\n",
      "    conditioning_key: crossattn\n",
      "    monitor: val/loss_simple_ema\n",
      "    scale_factor: 0.18215\n",
      "    use_ema: false\n",
      "    embedding_reg_weight: 0.0\n",
      "    unfreeze_model: true\n",
      "    model_lr: 1.0e-06\n",
      "    personalization_config:\n",
      "      target: ldm.modules.embedding_manager.EmbeddingManager\n",
      "      params:\n",
      "        placeholder_strings:\n",
      "        - '*'\n",
      "        initializer_words:\n",
      "        - sculpture\n",
      "        per_image_tokens: false\n",
      "        num_vectors_per_token: 1\n",
      "        progressive_words: false\n",
      "    unet_config:\n",
      "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
      "      params:\n",
      "        image_size: 32\n",
      "        in_channels: 4\n",
      "        out_channels: 4\n",
      "        model_channels: 320\n",
      "        attention_resolutions:\n",
      "        - 4\n",
      "        - 2\n",
      "        - 1\n",
      "        num_res_blocks: 2\n",
      "        channel_mult:\n",
      "        - 1\n",
      "        - 2\n",
      "        - 4\n",
      "        - 4\n",
      "        num_heads: 8\n",
      "        use_spatial_transformer: true\n",
      "        transformer_depth: 1\n",
      "        context_dim: 768\n",
      "        use_checkpoint: true\n",
      "        legacy: false\n",
      "    first_stage_config:\n",
      "      target: ldm.models.autoencoder.AutoencoderKL\n",
      "      params:\n",
      "        embed_dim: 4\n",
      "        monitor: val/rec_loss\n",
      "        ddconfig:\n",
      "          double_z: true\n",
      "          z_channels: 4\n",
      "          resolution: 512\n",
      "          in_channels: 3\n",
      "          out_ch: 3\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 2\n",
      "          - 4\n",
      "          - 4\n",
      "          num_res_blocks: 2\n",
      "          attn_resolutions: []\n",
      "          dropout: 0.0\n",
      "        lossconfig:\n",
      "          target: torch.nn.Identity\n",
      "    cond_stage_config:\n",
      "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
      "    ckpt_path: nai-animefull-final-pruned.ckpt\n",
      "data:\n",
      "  target: main.DataModuleFromConfig\n",
      "  params:\n",
      "    batch_size: 10\n",
      "    num_workers: 12\n",
      "    wrap: false\n",
      "    train:\n",
      "      target: ldm.data.personalized_batch.PersonalizedBatchBase\n",
      "      params:\n",
      "        size: 512\n",
      "        set: train\n",
      "        repeats: 100\n",
      "    validation:\n",
      "      target: ldm.data.personalized.PersonalizedBase\n",
      "      params:\n",
      "        size: 512\n",
      "        set: val\n",
      "        repeats: 10\n",
      "\n",
      "Lightning config\n",
      "modelcheckpoint:\n",
      "  params:\n",
      "    every_n_train_steps: 300\n",
      "callbacks:\n",
      "  image_logger:\n",
      "    target: main.ImageLogger\n",
      "    params:\n",
      "      batch_frequency: 150\n",
      "      max_images: 8\n",
      "      increase_log_steps: false\n",
      "trainer:\n",
      "  benchmark: true\n",
      "  max_steps: 5000\n",
      "  gpus: 0,\n",
      "\n",
      "Training: 0it [00:00, ?it/s]/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "Epoch 0:   0%|                                          | 0/425 [00:00<?, ?it/s]/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "Epoch 0:  35%|| 149/425 [09:19<17:16,  3.76s/it, loss=0.177, v_num=0, train/losData shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:03<02:57,  3.62s/it]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:03<01:18,  1.64s/it]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:04<00:47,  1.01s/it]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:04<00:32,  1.41it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:04<00:24,  1.83it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:04<00:19,  2.23it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:05<00:16,  2.60it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:05<00:14,  2.91it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:05<00:12,  3.16it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:05<00:11,  3.36it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:06<00:11,  3.51it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:06<00:10,  3.62it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:06<00:09,  3.71it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:06<00:09,  3.76it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:07<00:09,  3.81it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:07<00:08,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:07<00:08,  3.86it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:07<00:08,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:08<00:07,  3.89it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:08<00:07,  3.89it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:08<00:07,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:08<00:07,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:09<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:09<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:09<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:10<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:10<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:10<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:10<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:11<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:11<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:11<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:11<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:12<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:12<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:12<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:12<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:13<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:13<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:13<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:13<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:14<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:14<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:14<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:14<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:15<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:15<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:15<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:15<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:16<00:00,  3.10it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:03<03:14,  3.96s/it]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:04<01:31,  1.91s/it]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:04<00:59,  1.26s/it]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:05<00:43,  1.06it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:05<00:34,  1.29it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:06<00:29,  1.49it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:06<00:26,  1.65it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:07<00:23,  1.77it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:07<00:21,  1.87it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:08<00:20,  1.94it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:08<00:19,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:09<00:18,  2.02it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:09<00:18,  2.05it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:10<00:17,  2.07it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:10<00:16,  2.08it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:11<00:16,  2.09it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:11<00:15,  2.10it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:12<00:15,  2.10it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:12<00:14,  2.10it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:12<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:13<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:13<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:14<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:14<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:15<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:15<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:16<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:16<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:17<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:17<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:18<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:18<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:19<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:19<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:20<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:20<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:21<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:21<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:21<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:22<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:22<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:23<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:23<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:24<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:24<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:25<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:25<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:26<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:26<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:27<00:00,  1.84it/s]\u001b[A\n",
      "Epoch 0:  70%|| 299/425 [19:26<08:11,  3.90s/it, loss=0.185, v_num=0, train/lospop from empty list\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:12,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:12,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:00<00:12,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:01<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:02<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:02<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:02<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:02<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:03<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:03<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:03<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:03<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:04<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:04<00:08,  3.92it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:04<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:04<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:05<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:05<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:05<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:05<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:06<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:07<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:07<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:07<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:07<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:08<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:09<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:09<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:09<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:09<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:10<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:23,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:02<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:02<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:04<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:04<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:08<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:09<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:09<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:10<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:11<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:11<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:12<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:12<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:13<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:13<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:14<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:17<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:18<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:19<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:19<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:20<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:20<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:21<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:21<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:22<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:22<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "Epoch 0:  71%|| 300/425 [20:12<08:25,  4.04s/it, loss=0.185, v_num=0, train/los/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/loss_simple_ema')` could not find the monitored key in the returned metrics: ['train/loss_simple', 'train/loss_simple_step', 'train/loss_vlb', 'train/loss_vlb_step', 'train/loss', 'train/loss_step', 'global_step', 'epoch', 'step']. HINT: Did you call `log('val/loss_simple_ema', value)` in the `LightningModule`?\n",
      "  warning_cache.warn(m)\n",
      "Epoch 0, global step 300: 'val/loss_simple_ema' was not in top 10\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 300.\n",
      "Epoch 0:  99%|| 420/425 [27:41<00:19,  3.96s/it, loss=0.176, v_num=0, train/los\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|                | 1/5 [00:01<00:04,  1.08s/it]\u001b[A\n",
      "Epoch 0:  99%|| 421/425 [27:43<00:15,  3.95s/it, loss=0.176, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0:  40%|            | 2/5 [00:02<00:03,  1.08s/it]\u001b[A\n",
      "Epoch 0:  99%|| 422/425 [27:44<00:11,  3.95s/it, loss=0.176, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0:  60%|        | 3/5 [00:03<00:02,  1.08s/it]\u001b[A\n",
      "Epoch 0: 100%|| 423/425 [27:45<00:07,  3.94s/it, loss=0.176, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0:  80%|    | 4/5 [00:04<00:01,  1.08s/it]\u001b[A\n",
      "Epoch 0: 100%|| 424/425 [27:46<00:03,  3.93s/it, loss=0.176, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0: 100%|| 5/5 [00:08<00:00,  2.26s/it]\u001b[A\n",
      "Epoch 0: 100%|| 425/425 [27:51<00:00,  3.93s/it, loss=0.176, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[A/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2028: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2028: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.\n",
      "  rank_zero_deprecation(\n",
      "Average Epoch time: 1671.97 seconds\n",
      "Average Peak memory 35862.10MiB\n",
      "Epoch 0: 100%|| 425/425 [27:51<00:00,  3.93s/it, loss=0.176, v_num=0, train/losPrunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 420.\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "Epoch 1:   7%| | 29/425 [01:47<24:32,  3.72s/it, loss=0.175, v_num=0, train/losspop from empty list\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:12,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:12,  3.89it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:00<00:12,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:02<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:02<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:02<00:10,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:02<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:03<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:03<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:03<00:09,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:03<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:04<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:04<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:04<00:08,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:04<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:05<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:05<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:05<00:07,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:05<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:06<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:07<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:07<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:07<00:05,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:07<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:08<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:09<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:09<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:09<00:03,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:09<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:11<00:01,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:23,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:22,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:01<00:22,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:21,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:02<00:21,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:02<00:20,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:03<00:20,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:03<00:19,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:04<00:19,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:04<00:18,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:05<00:18,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:05<00:17,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:06<00:17,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:06<00:16,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:07<00:16,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:07<00:16,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:08<00:15,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:08<00:15,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:08<00:14,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:09<00:14,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:09<00:13,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:10<00:13,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:10<00:12,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:11<00:12,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:11<00:11,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:12<00:11,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:12<00:10,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:13<00:10,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:13<00:09,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:14<00:09,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:14<00:08,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:15<00:08,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:15<00:08,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:16<00:07,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:16<00:07,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:16<00:06,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:17<00:06,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:17<00:05,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:18<00:05,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:18<00:04,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:19<00:04,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:19<00:03,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:20<00:03,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:20<00:02,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:21<00:02,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:21<00:01,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:22<00:01,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:22<00:00,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:23<00:00,  2.12it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:23<00:00,  2.12it/s]\u001b[A\n",
      "Epoch 1:  42%|| 179/425 [11:41<16:04,  3.92s/it, loss=0.17, v_num=0, train/losspop from empty list\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:00<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:01<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:02<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:03<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:04<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:05<00:07,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:05<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:06<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:07<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:08<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:08<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:08<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:08<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:09<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:10<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:11<00:01,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:11<00:01,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:11<00:00,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:12<00:00,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:12<00:00,  3.95it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:12<00:00,  3.94it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:23,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:02<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:02<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:04<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:04<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:05<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:08<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:09<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:09<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:10<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:11<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:11<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:12<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:12<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:13<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:13<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:14<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:17<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:18<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:19<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:19<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:20<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:20<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:21<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:21<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:22<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:22<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "Epoch 1:  42%|| 180/425 [12:27<16:57,  4.15s/it, loss=0.166, v_num=0, train/losEpoch 1, global step 600: 'val/loss_simple_ema' reached 0.12165 (best 0.12165), saving model to '/home/beajun4958/jupyter/Kanewallmann-Stable-Diffusion/logs/ghislaine_smallset2022-11-08T21-59-52_ghislaine/checkpoints/epoch=000001.ckpt' as top 10\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 600.\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 600.\n",
      "Epoch 1:  77%|| 329/425 [21:51<06:22,  3.99s/it, loss=0.174, v_num=0, train/lospop from empty list\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:00<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:01<00:11,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:01<00:11,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:01<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:02<00:09,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:03<00:09,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:03<00:09,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:03<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:04<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:05<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:06<00:06,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:06<00:06,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:06<00:05,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:07<00:05,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:07<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:08<00:04,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:08<00:04,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:08<00:04,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:08<00:03,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:09<00:03,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:09<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:10<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:11<00:00,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:12<00:00,  3.95it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:12<00:00,  3.95it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:12<00:00,  3.94it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:23,  2.12it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:02<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:02<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:04<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:04<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:05<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:08<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:09<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:09<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:10<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:11<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:11<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:12<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:12<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:13<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:13<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:17<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:18<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:19<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:19<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:20<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:20<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:21<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:21<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:22<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:22<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "Epoch 1:  99%|| 420/425 [28:08<00:20,  4.02s/it, loss=0.164, v_num=0, train/los\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|                | 1/5 [00:01<00:04,  1.08s/it]\u001b[A\n",
      "Epoch 1:  99%|| 421/425 [28:10<00:16,  4.01s/it, loss=0.164, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0:  40%|            | 2/5 [00:02<00:03,  1.08s/it]\u001b[A\n",
      "Epoch 1:  99%|| 422/425 [28:11<00:12,  4.01s/it, loss=0.164, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0:  60%|        | 3/5 [00:03<00:02,  1.08s/it]\u001b[A\n",
      "Epoch 1: 100%|| 423/425 [28:12<00:08,  4.00s/it, loss=0.164, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0:  80%|    | 4/5 [00:04<00:01,  1.08s/it]\u001b[A\n",
      "Epoch 1: 100%|| 424/425 [28:13<00:03,  3.99s/it, loss=0.164, v_num=0, train/los\u001b[A\n",
      "Validation DataLoader 0: 100%|| 5/5 [00:04<00:00,  1.19it/s]\u001b[A\n",
      "Epoch 1: 100%|| 425/425 [28:14<00:00,  3.99s/it, loss=0.164, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[A/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2028: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2028: LightningDeprecationWarning: `Trainer.training_type_plugin` is deprecated in v1.6 and will be removed in v1.8. Use `Trainer.strategy` instead.\n",
      "  rank_zero_deprecation(\n",
      "Average Epoch time: 1694.37 seconds\n",
      "Average Peak memory 35862.94MiB\n",
      "Epoch 1: 100%|| 425/425 [28:14<00:00,  3.99s/it, loss=0.164, v_num=0, train/losPrunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 840.\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "/home/beajun4958/anaconda3/envs/jupy/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2102: LightningDeprecationWarning: `Trainer.root_gpu` is deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.strategy.root_device.index` instead.\n",
      "  rank_zero_deprecation(\n",
      "Epoch 2:  14%|| 59/425 [03:38<22:32,  3.70s/it, loss=0.17, v_num=0, train/loss_pop from empty list\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:00<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:01<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:02<00:10,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:02<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:03<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:04<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:05<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:06<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:07<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:08<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:08<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:08<00:04,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:08<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:09<00:03,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:09<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:10<00:02,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:10<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:11<00:01,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:11<00:00,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:12<00:00,  3.94it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:12<00:00,  3.94it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:12<00:00,  3.94it/s]\u001b[A\n",
      "Data shape for DDIM sampling is (8, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 50 timesteps\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "DDIM Sampler:   2%|                             | 1/50 [00:00<00:23,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   4%|                            | 2/50 [00:00<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   6%|                            | 3/50 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:   8%|                           | 4/50 [00:01<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  10%|                           | 5/50 [00:02<00:21,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  12%|                          | 6/50 [00:02<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  14%|                         | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  16%|                         | 8/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  18%|                        | 9/50 [00:04<00:19,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  20%|                       | 10/50 [00:04<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  22%|                      | 11/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  24%|                      | 12/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  26%|                     | 13/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  28%|                     | 14/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  30%|                    | 15/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  32%|                   | 16/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  34%|                   | 17/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  36%|                  | 18/50 [00:08<00:15,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  38%|                  | 19/50 [00:09<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  40%|                 | 20/50 [00:09<00:14,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  42%|                | 21/50 [00:09<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  44%|                | 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  46%|               | 23/50 [00:10<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  48%|               | 24/50 [00:11<00:12,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  50%|              | 25/50 [00:11<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  52%|              | 26/50 [00:12<00:11,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  54%|             | 27/50 [00:12<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  56%|            | 28/50 [00:13<00:10,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  58%|            | 29/50 [00:13<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  60%|           | 30/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  62%|           | 31/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  64%|          | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  66%|         | 33/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  68%|         | 34/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  70%|        | 35/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  72%|        | 36/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  74%|       | 37/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  76%|       | 38/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  78%|      | 39/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  80%|     | 40/50 [00:18<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  82%|     | 41/50 [00:19<00:04,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  84%|    | 42/50 [00:19<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  86%|    | 43/50 [00:20<00:03,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  88%|   | 44/50 [00:20<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  90%|   | 45/50 [00:21<00:02,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  92%|  | 46/50 [00:21<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  94%| | 47/50 [00:22<00:01,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  96%| | 48/50 [00:22<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler:  98%|| 49/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "DDIM Sampler: 100%|| 50/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "Epoch 2:  14%|| 60/425 [04:23<26:45,  4.40s/it, loss=0.173, v_num=0, train/lossEpoch 2, global step 900: 'val/loss_simple_ema' reached 0.07676 (best 0.07676), saving model to '/home/beajun4958/jupyter/Kanewallmann-Stable-Diffusion/logs/ghislaine_smallset2022-11-08T21-59-52_ghislaine/checkpoints/epoch=000002.ckpt' as top 10\n",
      "Prunin' Checkpoint\n",
      "Checkpoint Keys: dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "Removing optimizer states from checkpoint\n",
      "This is global step 900.\n"
     ]
    }
   ],
   "source": [
    "# START THE TRAINING\n",
    "project_name = \"ghislaine\"\n",
    "\n",
    "# MAX STEPS\n",
    "max_training_steps = 5000\n",
    "\n",
    "reg_data_root = \"/home/beajun4958/jupyter/Kanewallmann-Stable-Diffusion/regularization_images\"\n",
    "\n",
    "!rm -rf training_samples/.ipynb_checkpoints\n",
    "!python \"main.py\" \\\n",
    " --base configs/stable-diffusion/v1-finetune_unfrozen-40gb.yaml \\\n",
    " -t \\\n",
    " --actual_resume \"nai-animefull-final-pruned.ckpt\" \\\n",
    " --reg_data_root {reg_data_root} \\\n",
    " -n {project_name} \\\n",
    " --gpus 0, \\\n",
    " --data_root \"/home/beajun4958/jupyter/Kanewallmann-Stable-Diffusion/ghislaine_smallset/\" \\\n",
    " --max_training_steps {max_training_steps} \\\n",
    " --no-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49d0bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pruning (12GB to 2GB)\n",
    "We are working on having this happen automatically (TODO: PR's welcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618147f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_paths = !ls -d logs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2387c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prunin' in path: logs/nigiri_project2022-11-02T02-58-29_nigiri_usagi/checkpoints/last.ckpt\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n",
      "removing optimizer states for path logs/nigiri_project2022-11-02T02-58-29_nigiri_usagi/checkpoints/last.ckpt\n",
      "This is global step 2520.\n",
      "saving pruned checkpoint at: logs/nigiri_project2022-11-02T02-58-29_nigiri_usagi/checkpoints/last-pruned.ckpt\n",
      "New ckpt size: 2.13 GB. Saved 9.99 GB by removing optimizer states\n"
     ]
    }
   ],
   "source": [
    "# This version should automatically prune around 10GB from the ckpt file\n",
    "last_checkpoint_file = directory_paths[-1] + \"/checkpoints/last.ckpt\"\n",
    "!python \"prune_ckpt.py\" --ckpt {last_checkpoint_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf78ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint_file_pruned = directory_paths[-1] + \"/checkpoints/last-pruned.ckpt\"\n",
    "training_samples = !ls training_samples\n",
    "date_string = !date +\"%Y-%m-%dT%H-%M-%S\"\n",
    "file_name = date_string[-1] + \"_\" + project_name + \"_\" + str(len(training_samples)) + \"_training_images_\" +  str(max_training_steps) + \"_max_training_steps.ckpt\"\n",
    "!mkdir -p trained_models\n",
    "!mv {last_checkpoint_file_pruned} trained_models/{file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download your trained model file from `trained_models` and use in your favorite Stable Diffusion repo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90ac5c",
   "metadata": {},
   "source": [
    "# Big Important Note!\n",
    "\n",
    "The way to use your token is `<token> <class>` ie `joepenna person` and not just `joepenna`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d0139",
   "metadata": {},
   "source": [
    "## Generate Images With Your Trained Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ddb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/stable_txt2img.py \\\n",
    " --ddim_eta 0.0 \\\n",
    " --n_samples 1 \\\n",
    " --n_iter 4 \\\n",
    " --scale 7.0 \\\n",
    " --ddim_steps 50 \\\n",
    " --ckpt \"/workspace/Dreambooth-Stable-Diffusion/trained_models/\" + {file_name} \\\n",
    " --prompt \"joepenna person as a masterpiece portrait painting by John Singer Sargent in the style of Rembrandt\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
